# Quantization Module

> **Method C Implementation**: Direct FP8 quantization of Biomni-R0-32B with domain-specific calibration.

This module contains the implementation of **Method C (Direct Quantization)** from the thesis, which quantizes the domain-adapted Biomni-R0-32B model directly to FP8 using post-training quantization with biomedical calibration data.

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Module Structure](#-module-structure)
- [Calibration Data](#-calibration-data)
- [FP8 Quantization](#-fp8-quantization)
- [Installation](#-installation)
- [Usage](#-usage)
- [Logs](#-logs)

---

## ğŸ¯ Overview

Method C directly quantizes the Biomni-R0-32B model to FP8 format using the **llm-compressor** library with domain-specific calibration data from successful biomedical reasoning trajectories.

### Key Innovation: Deep Calibration

Unlike standard quantization recipes that use short context windows (512-1024 tokens), our calibration captures **full-trajectory activation patterns**:

| Metric | Value |
|--------|-------|
| Total Instances | 123 |
| Total Tokens | 3,163,274 |
| Average Context | 25,718 tokens |
| Maximum Context | 75,508 tokens |
| Sampling | Stratified across 10 Eval1 tasks |

---

## ğŸ“ Module Structure

\`\`\`
quantization/
â”œâ”€â”€ quantization_requirements.txt     # Python dependencies
â”œâ”€â”€ README.md                         # This file
â”‚
â”œâ”€â”€ ğŸ“‚ calibration_data/              # Biomedical calibration dataset
â”‚   â”œâ”€â”€ Data_r0_annotated.jsonl       # Raw successful trajectories
â”‚   â”œâ”€â”€ Data_r0_annotated_cleaned.jsonl  # Cleaned calibration data
â”‚   â”œâ”€â”€ calibration_data.json         # Final calibration format
â”‚   â”œâ”€â”€ calibration_preview.txt       # Sample preview
â”‚   â”œâ”€â”€ prepare_calibration.py        # Data preparation script
â”‚   â””â”€â”€ clean_calibration_data.py     # Data cleaning script
â”‚
â””â”€â”€ ğŸ“‚ scripts/
    â”œâ”€â”€ ğŸ“‚ FP8_quantization/          # Method C implementation
    â”‚   â”œâ”€â”€ quantize_FP8.py           # Main quantization script
    â”‚   â”œâ”€â”€ quanitze_FP8.sh           # SLURM job script
    â”‚   â”œâ”€â”€ quantize_FP8.log          # Execution log
    â”‚   â””â”€â”€ quantize_FP8.err          # Error log
    â”‚
    â””â”€â”€ ğŸ“‚ INT4_quantization/         # AWQ INT4 (exploratory)
        â”œâ”€â”€ quantize_AWQ_INT4.py      # INT4 quantization script
        â”œâ”€â”€ quanitze_INT4.sh          # SLURM job script
        â””â”€â”€ quantize_AWQ_INT4_*.log   # Execution logs
\`\`\`

---

## ğŸ“Š Calibration Data

Calibration data is derived from **successful biomedical reasoning trajectories** on the Eval1 benchmark with stratified sampling across all 10 tasks.

### Data Preparation

\`\`\`bash
cd calibration_data
python prepare_calibration.py --input ../results.jsonl --output calibration_data.json
\`\`\`

---

## âš¡ FP8 Quantization

### Configuration

- **Format**: FP8 E4M3 with Block-128 quantization
- **Excluded Layers**: lm_head (kept in higher precision)
- **Max Sequence Length**: 76,000 tokens

### Usage

\`\`\`bash
cd scripts/FP8_quantization
sbatch quanitze_FP8.sh  # SLURM
# OR
python quantize_FP8.py  # Direct
\`\`\`

---

## ğŸ“‹ Logs

| Log File | Description |
|----------|-------------|
| \`scripts/FP8_quantization/quantize_FP8.log\` | FP8 quantization output |
| \`scripts/FP8_quantization/quantize_FP8.err\` | Error messages |
| \`scripts/INT4_quantization/quantize_AWQ_INT4_*.log\` | INT4 logs |

---

## ğŸ“š References

- llm-compressor: https://github.com/neuralmagic/llm-compressor
- FP8 Training: https://arxiv.org/abs/2209.05433
