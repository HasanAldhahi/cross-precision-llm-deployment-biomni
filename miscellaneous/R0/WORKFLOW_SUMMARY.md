# Biomni-R0 LoRA Extraction & Fine-tuning Workflow

## Quick Start (3 Steps)

### Step 1: Extract LoRA
```bash
python Compare_R0_qwen3.py
```

### Step 2: Analyze Results
```bash
python analyze_lora_results.py
```

### Step 3: Fine-tune
```bash
python finetune_with_lora.py \
    --lora-weights model_comparison/extracted_lora_weights.safetensors \
    --dataset your/dataset
```

## Complete Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LORA EXTRACTION WORKFLOW                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                                  START
                                    â”‚
                                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  1. Compare Models                       â”‚
          â”‚  python Compare_R0_qwen3.py              â”‚
          â”‚                                          â”‚
          â”‚  Downloads:                              â”‚
          â”‚  â€¢ Qwen/Qwen3-32B-FP8 (base)            â”‚
          â”‚  â€¢ biomni/Biomni-R0-32B-Preview (FT)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Comparison Analysis                     â”‚
          â”‚                                          â”‚
          â”‚  âœ“ Config differences                   â”‚
          â”‚  âœ“ Architecture comparison              â”‚
          â”‚  âœ“ Weight differences                   â”‚
          â”‚  âœ“ LoRA pattern detection               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Found LoRA?    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚              â”‚
                    YES   â”‚              â”‚   NO
                          â–¼              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Explicit      â”‚   â”‚ Low-rank     â”‚
              â”‚ LoRA Layers   â”‚   â”‚ Patterns     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚              â”‚
                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  2. Analyze Results                      â”‚
          â”‚  python analyze_lora_results.py          â”‚
          â”‚                                          â”‚
          â”‚  Shows:                                  â”‚
          â”‚  â€¢ Modified layers                       â”‚
          â”‚  â€¢ LoRA rank recommendations             â”‚
          â”‚  â€¢ Target modules                        â”‚
          â”‚  â€¢ Next steps                            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  3. Fine-tune with LoRA                  â”‚
          â”‚  python finetune_with_lora.py            â”‚
          â”‚                                          â”‚
          â”‚  Options:                                â”‚
          â”‚  â€¢ Use extracted weights                 â”‚
          â”‚  â€¢ Use detected config                   â”‚
          â”‚  â€¢ Fresh LoRA on FT model               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Outputs                                 â”‚
          â”‚                                          â”‚
          â”‚  â€¢ LoRA adapter (small, shareable)       â”‚
          â”‚  â€¢ Merged model (full model)             â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  4. Deploy & Evaluate                    â”‚
          â”‚                                          â”‚
          â”‚  â€¢ Serve with SGLang/vLLM                â”‚
          â”‚  â€¢ Use with Biomni A1 agent              â”‚
          â”‚  â€¢ Evaluate on biomedical benchmarks     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                                   END
```

## Files Generated

### Comparison Results (`model_comparison/`)
```
model_comparison/
â”œâ”€â”€ config_comparison.json          # Configuration differences
â”œâ”€â”€ architecture_comparison.json    # Layer structure analysis
â”œâ”€â”€ weight_differences.json         # Weight modification details
â”œâ”€â”€ lora_analysis.json             # LoRA pattern detection results
â”œâ”€â”€ lora_metadata.json             # LoRA configuration info
â”œâ”€â”€ extracted_lora_weights.safetensors  # Ready-to-use LoRA weights
â””â”€â”€ comparison_report.txt          # Human-readable summary
```

### Fine-tuned Model (`finetuned_model/`)
```
finetuned_model/
â”œâ”€â”€ lora_adapter/                  # LoRA adapter only (~100MB)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ tokenizer files
â””â”€â”€ merged_model/                  # Full merged model (~64GB)
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â””â”€â”€ tokenizer files
```

## Three Scenarios

### ğŸ¯ Scenario A: Explicit LoRA Found
```
architecture_comparison.json shows:
  "lora_keys": [
    "model.layers.0.self_attn.q_proj.lora_A",
    "model.layers.0.self_attn.q_proj.lora_B",
    ...
  ]
```

**What this means**: The model has dedicated LoRA adapter layers

**Action**:
```bash
# Extract the LoRA layers directly
python Compare_R0_qwen3.py

# Use them for fine-tuning
python finetune_with_lora.py \
    --lora-weights model_comparison/extracted_lora_weights.safetensors
```

### âš™ï¸ Scenario B: LoRA Patterns Detected
```
lora_analysis.json shows:
  "potential_lora_layers": [...],
  "analysis": {
    "layer_name": {
      "rank_95_energy": 24,
      "is_potential_lora": true
    }
  }
```

**What this means**: Weight differences show low-rank patterns

**Action**:
```bash
# Extracted weights available via SVD decomposition
python finetune_with_lora.py \
    --lora-weights model_comparison/extracted_lora_weights.safetensors \
    --rank 32
```

### ğŸ“š Scenario C: Full Fine-tuning
```
weight_differences.json shows:
  Most layers have high modification ratios
  No low-rank patterns detected
```

**What this means**: All parameters were modified during training

**Action**:
```bash
# Apply fresh LoRA on top of the finetuned model
python finetune_with_lora.py \
    --base-model biomni/Biomni-R0-32B-Preview \
    --rank 16 \
    --target-modules q_proj k_proj v_proj o_proj
```

## Key Scripts

| Script | Purpose | Input | Output |
|--------|---------|-------|--------|
| `Compare_R0_qwen3.py` | Model comparison & LoRA extraction | Model names | Comparison results + LoRA weights |
| `analyze_lora_results.py` | Visualize & summarize results | Comparison dir | Analysis report |
| `finetune_with_lora.py` | Fine-tune with LoRA | LoRA weights + dataset | Fine-tuned model |
| `run_R0.py` | Serve & evaluate model | Model path | Evaluation results |

## Common Commands

### Basic Workflow
```bash
# 1. Extract LoRA
python Compare_R0_qwen3.py

# 2. Analyze
python analyze_lora_results.py

# 3. Fine-tune
python finetune_with_lora.py \
    --lora-weights model_comparison/extracted_lora_weights.safetensors \
    --dataset biomni/your-dataset

# 4. Evaluate
python run_R0.py \
    --base-url http://localhost:30000 \
    --model-path ./finetuned_model/merged_model
```

### Custom Configurations
```bash
# Higher rank extraction
python Compare_R0_qwen3.py --lora-rank 32

# Selective fine-tuning
python finetune_with_lora.py \
    --target-modules q_proj k_proj v_proj \
    --rank 16

# Different base model
python Compare_R0_qwen3.py \
    --base Qwen/Qwen3-32B \
    --finetuned biomni/Biomni-R0-32B-Preview
```

### Multi-task Learning
```bash
# Task 1: CRISPR
python finetune_with_lora.py \
    --lora-weights model_comparison/extracted_lora_weights.safetensors \
    --dataset biomni/crispr-data \
    --output-dir ./models/crispr

# Task 2: Drug discovery (continue from Task 1)
python finetune_with_lora.py \
    --lora-weights ./models/crispr/lora_adapter \
    --dataset biomni/drug-discovery \
    --output-dir ./models/drug-discovery
```

## Parameter Guide

### LoRA Rank Selection
- **Rank 8**: Quick experiments, minimal parameters
- **Rank 16**: Standard (recommended for most cases)
- **Rank 32**: Higher fidelity, more parameters
- **Rank 64**: Maximum quality, largest adapter

### Target Modules
```python
# Attention-only (faster, less parameters)
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

# Full (better performance, more parameters)
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
    "gate_proj", "up_proj", "down_proj"      # FFN
]
```

### Training Hyperparameters
```bash
# Conservative (stable)
--learning-rate 1e-4 --batch-size 4 --epochs 3

# Aggressive (faster, less stable)
--learning-rate 5e-4 --batch-size 8 --epochs 5

# Production (balanced)
--learning-rate 2e-4 --batch-size 4 --epochs 3
```

## GPU Requirements

| Task | GPU Memory | Recommended GPUs |
|------|-----------|------------------|
| Model comparison | ~80GB | 1Ã—A100 or 2Ã—A40 |
| LoRA fine-tuning (rank 16) | ~40GB | 1Ã—A100 or 1Ã—A40 |
| LoRA fine-tuning (rank 64) | ~60GB | 1Ã—A100 |
| Full model inference | ~70GB | 1Ã—A100 or 2Ã—A40 |

## Troubleshooting Decision Tree

```
Issue: Out of Memory
â”œâ”€ During comparison? â†’ Use smaller threshold (--threshold 1e-7)
â”œâ”€ During fine-tuning? â†’ Reduce batch size (--batch-size 1)
â””â”€ During inference? â†’ Use quantization (FP8/FP16)

Issue: No LoRA Found
â”œâ”€ Try lower threshold â†’ --threshold 1e-8
â”œâ”€ Check specific layers â†’ analyze_lora_results.py
â””â”€ Model fully fine-tuned â†’ Apply fresh LoRA on top

Issue: Poor Performance
â”œâ”€ Increase LoRA rank â†’ --rank 32 or --rank 64
â”œâ”€ Add more target modules â†’ Include FFN layers
â””â”€ More training â†’ --epochs 5 --learning-rate 1e-4

Issue: Model Loading Error
â”œâ”€ Check HF token â†’ huggingface-cli login
â”œâ”€ Download manually â†’ huggingface-cli download <model>
â””â”€ Check disk space â†’ df -h
```

## Best Practices Checklist

- [ ] Run comparison first to understand model differences
- [ ] Start with low rank (8-16) and increase if needed
- [ ] Monitor reconstruction error in LoRA extraction
- [ ] Use biomedical datasets for biomedical models
- [ ] Validate on held-out biomedical benchmarks
- [ ] Save checkpoints during training
- [ ] Track experiments (wandb/tensorboard)
- [ ] Test with A1 agent before deployment
- [ ] Document your modifications
- [ ] Share LoRA adapters (smaller than full models)

## Additional Resources

- Full guide: [LORA_EXTRACTION_GUIDE.md](LORA_EXTRACTION_GUIDE.md)
- Evaluation: [README.md](README.md)
- Quick start: [QUICKSTART.md](QUICKSTART.md)
- Biomni-R0: https://huggingface.co/biomni/Biomni-R0-32B-Preview
- Qwen3-32B-FP8: https://huggingface.co/Qwen/Qwen3-32B-FP8


