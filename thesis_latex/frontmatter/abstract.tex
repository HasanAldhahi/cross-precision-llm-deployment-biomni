Running Large Language Models (LLMs) in hardware-constrained settings requires ever-greater levels of aggressive compression; in particular, 8-bit floating-point (FP8) is now emerging as a standard for reducing memory costs. Yet despite the rapid adoption of 8-bit quantization in production, a large technical gap remains: domain-specific adaptations are always assembled in FP16/BF16 precision, creating a precision mismatch whenever those patches are applied over a pre-quantized FP8 base. This thesis seeks to answer whether high-precision Low-Rank Adaptation (LoRA) modules can be successfully `transplanted' onto FP8 base models without retraining, and thus be used on existing shared systems to run domain-specific biomedical adapters.

We designed and prototyped three approaches: (1) Naive Transfer, (2) Corrective Extraction, and (3) Direct Quantization. Testing all three on the Eval1 biomedical benchmark---a challenging test set of 433 questions on CRISPR, GWAS, and rare disease diagnostics---we obtained surprising results: despite the expectation that accuracy should suffer under precision loss, Naive Transfer actually achieved 44.6\% accuracy, matching our 44.5\% full-precision baseline.

This evidence supports the \textit{Orthogonality Hypothesis}: the observation that the effect of quantization noise (high-rank, isotropic) is orthogonal to the semantic information encoded in LoRA (low-rank, structured). A key finding was the near-zero cosine similarity on average across all model layers ($\approx -0.00001$) between the LoRA adapter vector and the quantization noise vector, and it turns out this high-dimensional weight space is able to accommodate both parts without functional interference.

Practical implications include a scalable way to enable resource-efficient AI on existing infrastructure. An extension to vLLM's multi-adapter system allows us to incorporate our specialized biomedical tools like Biomni into existing Chat AI infrastructure, requiring no additional GPU cost, by simply loading the LoRA adapter alongside the pre-quantized Qwen-32B-FP8 base model. This allows domain-specific AI services to share GPU resources with general-purpose models, reducing the hardware and operational costs of running high-throughput biomedical AI services.
