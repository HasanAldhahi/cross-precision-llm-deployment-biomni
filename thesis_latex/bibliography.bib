@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{micikevicius2022fp8,
  title={FP8 Formats for Deep Learning},
  author={Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and others},
  journal={arXiv preprint arXiv:2209.05433},
  year={2022}
}

@article{yang2025qwen3,
  title={Qwen3 Technical Report},
  author={Yang, A. and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Roziere, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{gholami2021survey,
  title={A Survey of Quantization Methods for Efficient Neural Network Inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Low-Power Computer Vision},
  pages={291--326},
  year={2021},
  organization={Chapman and Hall/CRC}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}

@article{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{lee2018biobert,
  title={BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020}
}

@article{singhal2023large,
  title={Large Language Models Encode Clinical Knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  pages={172--180},
  year={2023}
}

@article{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  journal={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  journal={arXiv preprint arXiv:2309.06180},
  year={2023}
}

@misc{mergekit2023,
  title={MergeKit: Tools for Merging Pre-trained Language Models},
  author={Goddard, Charles and Siriwardhana, Shamane and Ehghaghi, Malikeh and Meyers, Luke and Karpukhin, Vladimir and Benedict, Brian and McQuade, Mark and Solawetz, Jacob},
  howpublished={\url{https://github.com/cg123/mergekit}},
  year={2023},
  note={Accessed: 2026-02-02}
}

@misc{llmcompressor2024,
  title={LLM Compressor: A Toolkit for Neural Network Compression},
  author={Neural Magic},
  howpublished={\url{https://github.com/vllm-project/llm-compressor}},
  year={2024},
  note={Accessed: 2026-02-02}
}

@article{biomni2025,
  title={Biomni: A Generalist Biomedical AI Agent},
  author={Huang, Kexin and Chandak, Payal and Zhang, Qianwen and Moor, Michael and Leskovec, Jure and Zitnik, Marinka},
  journal={bioRxiv preprint},
  doi={10.1101/2025.05.30.656746},
  year={2025},
  note={Stanford University. Platform and benchmark paper.}
}

@misc{biomnir0,
  title     = {Biomni-R0: Using RL to Hill-Climb Biomedical Reasoning Agents to Expert-Level},
  author    = {Ryan Li and Kexin Huang and Shiyi Cao and Yuanhao Qu and Jure Leskovec},
  year      = {2025},
  month     = {9},
  note      = {Technical Report. Model-specific paper for the R0 variant.}
}

@misc{gwdg2024chatai,
  title={Chat AI: Secure LLM Web Service},
  author={{GWDG - Gesellschaft fuer wissenschaftliche Datenverarbeitung mbH Goettingen}},
  howpublished={\url{https://docs.hpc.gwdg.de/services/chat-ai/}},
  year={2024},
  note={Accessed: 2026-02-02}
}

@software{langgraph2024,
  title={LangGraph: Build Stateful, Multi-Actor Applications with LLMs},
  author={{LangChain Inc.}},
  url={https://github.com/langchain-ai/langgraph},
  version={0.2},
  year={2024},
  note={Accessed: 2026-02-02}
}

@inproceedings{dettmers2022llmint8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@inproceedings{dettmers2023blockwise,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  booktitle={9th International Conference on Learning Representations},
  year={2022}
}

@inproceedings{li2024loftq,
  title={LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models},
  author={Li, Yixiao and Yu, Yifan and Liang, Chen and Karampatziakis, Nikos and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{luo2022biogpt,
  title={BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining},
  author={Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={6},
  year={2022}
}

@article{lee2024rilq,
  title={RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy},
  author={Lee, Geon and Lee, Jaehyeong and Hong, Seokmin and Ahn, Eunji and Chang, Dongsoo and Choi, Jinho},
  journal={arXiv preprint arXiv:2412.01129},
  year={2024}
}

@misc{vllmpr33234,
  title={vLLM LoRA Module Support Limitations},
  author={{vLLM Project Contributors}},
  howpublished={\url{https://github.com/vllm-project/vllm/pull/33234}},
  year={2024},
  note={GitHub Pull Request \#33234. Accessed: 2026-02-02}
}

@misc{biomni_r0_32b_preview_gguf,
  author = {mradermacher},
  title = {mradermacher/Biomni-R0-32B-Preview-GGUF},
  year = {2025},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/mradermacher/Biomni-R0-32B-Preview-GGUF}},
  note = {Accessed: 2026-02-02}
}

@dataset{biomni_eval1,
  author = {Biomni team},
  title = {biomni/Eval1},
  publisher = {Hugging Face},
  year = {2025},
  url = {https://huggingface.co/datasets/biomni/Eval1},
  note = {Accessed: 2026-02-02}
}