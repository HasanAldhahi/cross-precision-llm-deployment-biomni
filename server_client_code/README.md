# Server Module

> **Model Inference Infrastructure**: vLLM-based serving with multi-GPU load balancing for biomedical LLM evaluation.

This module provides the infrastructure for deploying and evaluating the quantized models using vLLM with multi-GPU support.

---

## üìã Table of Contents

- [Overview](#-overview)
- [Module Structure](#-module-structure)
- [Server Architecture](#-server-architecture)
- [Installation](#-installation)
- [Usage](#-usage)
- [Logs](#-logs)

---

## üéØ Overview

The server module enables high-throughput inference for evaluating different model configurations:

| Feature | Description |
|---------|-------------|
| **Backend** | vLLM with FP8/LoRA support |
| **Multi-GPU** | 4x GPU load balancing |
| **Endpoints** | OpenAI-compatible API |
| **Evaluation** | Eval1 biomedical benchmark |

---

## üìÅ Module Structure

```
server/
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ README.md                     # This file
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_server.sh             # Main server launch script
‚îÇ   ‚îú‚îÄ‚îÄ run_client.sh             # Evaluation client launcher
‚îÇ   ‚îú‚îÄ‚îÄ run_client_Eval1_benchmark.py  # Eval1 benchmark runner
‚îÇ   ‚îú‚îÄ‚îÄ load_balancer.py          # Multi-GPU request distributor
‚îÇ   ‚îî‚îÄ‚îÄ diagnose_servers.sh       # Health check utility
‚îÇ
‚îú‚îÄ‚îÄ üìÇ Documentation
‚îÇ   ‚îî‚îÄ‚îÄ LOAD_BALANCER_FIX.md      # Load balancer troubleshooting
‚îÇ
‚îî‚îÄ‚îÄ üìÇ Logs
    ‚îú‚îÄ‚îÄ vllm_gpu0.log             # GPU 0 server log
    ‚îú‚îÄ‚îÄ vllm_gpu1.log             # GPU 1 server log
    ‚îú‚îÄ‚îÄ vllm_gpu2.log             # GPU 2 server log
    ‚îú‚îÄ‚îÄ vllm_gpu3.log             # GPU 3 server log
    ‚îî‚îÄ‚îÄ load_balancer.log         # Load balancer log
```

---

## üèóÔ∏è Server Architecture

### Multi-GPU Setup

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Load Balancer                  ‚îÇ
‚îÇ              (Round-Robin Distribution)         ‚îÇ
‚îÇ                  Port: 8000                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ           ‚îÇ           ‚îÇ           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ vLLM   ‚îÇ  ‚îÇ vLLM   ‚îÇ  ‚îÇ vLLM   ‚îÇ  ‚îÇ vLLM   ‚îÇ
        ‚îÇ GPU 0  ‚îÇ  ‚îÇ GPU 1  ‚îÇ  ‚îÇ GPU 2  ‚îÇ  ‚îÇ GPU 3  ‚îÇ
        ‚îÇ :8001  ‚îÇ  ‚îÇ :8002  ‚îÇ  ‚îÇ :8003  ‚îÇ  ‚îÇ :8004  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Supported Model Configurations

| Configuration | Command Flag | Memory/GPU |
|---------------|--------------|------------|
| R0-32B-BF16 | `--model r0-bf16` | ~17GB |
| R0-32B-FP8 (Method C) | `--model r0-fp8` | ~9GB |
| Qwen-FP8 + LoRA (Method A) | `--model qwen-fp8-lora` | ~10GB |

---

## üîß Installation

```bash
pip install -r requirements.txt
```

### Key Dependencies

- `vllm>=0.4.0` - High-performance LLM serving
- `openai` - API client for evaluation
- `aiohttp` - Async HTTP for load balancer
- `python-dotenv` - Environment variable management

---

## üíª Usage

### Start Server

```bash
# Full precision baseline
bash run_server.sh --model r0-bf16 --gpus 4

# Method C: Direct quantization
bash run_server.sh --model r0-fp8 --gpus 4

# Method A: FP8 + LoRA
bash run_server.sh --model qwen-fp8-lora --gpus 4 \
    --lora-path ../brain_surgery/lora_extraction_results/Method_A_lora_basic_original_base_rank_256
```

### Run Evaluation

```bash
bash run_client.sh --benchmark eval1
# OR
python run_client_Eval1_benchmark.py --server http://localhost:8000
```

### Diagnose Issues

```bash
bash diagnose_servers.sh
```

---

## üìã Logs

| Log | Description |
|-----|-------------|
| `vllm_gpu{0-3}.log` | Per-GPU vLLM server output |
| `load_balancer.log` | Request distribution logs |

Logs contain:
- Model loading progress
- Request/response latencies
- Error traces
- Memory usage statistics

---

## ‚ö†Ô∏è Troubleshooting

See [LOAD_BALANCER_FIX.md](LOAD_BALANCER_FIX.md) for common issues and solutions.
